import numpy as np
import seaborn as sns
import tensorflow as tf
from matplotlib import pyplot as plt
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
import cv2
import io
import base64

def classes_acc_testss(X_test, y_test, model, comm_round, tp_div_support_dict):

    predict_labels = model.predict(X_test)
    pred_labels = np.argmax(predict_labels, axis=1)
    true_labels = np.argmax(y_test, axis=1)
    # calculate the classification report for the i-th iteration
    report = classification_report(true_labels, pred_labels, output_dict=True, zero_division=1)

    # calculate the confusion matrix for the i-th iteration
    cm = confusion_matrix(true_labels, pred_labels)

    # calculate TP divided by support for each label for the i-th iteration
    for j in range(10):
        support_j = report[str(j)]['support']  # support for Label j
        tp_j = cm[j, j]  # TP for Label j
        tp_div_support_j = tp_j / support_j
        tp_div_support_dict[comm_round,j] = tp_div_support_j

    return tp_div_support_dict


#The purpose of this function is to evaluate a machine learning model on a set of test data.

def test_model(X_test, y_test, model, comm_round, which_model, client_name):                     #comm_round = the iteration number of the training process

    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)     #enable logits as inputs to the loss function
    #logits = model.predict(X_test, batch_size=100)
    logits = model.predict(X_test)         #raw, unscaled predictions generated by the model just before the final activation function.
    #loss function
    loss = cce(y_test, logits)
    #Calculate the accuracy of the model's predictions by comparing the predicted class labels (obtained by taking the argmax of the predicted class probabilities) with the true class labels.
    acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(y_test, axis=1))

    if (which_model=='localModel'):
        print('comm_round: {} | client_name: {} | local_acc: {:.3%} | local_loss: {}'.format(comm_round, client_name , acc, loss))
        return acc, loss
    else:
        print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss))
        return acc, loss

def confusion_matrix_acc(global_model,X_test,Y_test):

    Y_predic = global_model.predict(X_test)
    Y_pred_labels = [np.argmax(i) for i in Y_predic]
    Y_test_labels = [np.argmax(i) for i in Y_test]
    conf_mat = confusion_matrix(Y_test_labels, Y_pred_labels)
    plt.figure(figsize=(15,7))
    sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues')
    plt.ylabel('True Labels')
    plt.xlabel('Predicted Labels')
    plt.savefig('foo.png')
    with open("foo.png", "rb") as f:
        image_bytes = f.read()
    return base64.b64encode(image_bytes).decode()
    
    
    

# plot predict scores for each label
def predic_acc(tp_div_support_dict,iterations):

    plt.figure(figsize=(10,6))
    plt.ylim(0, 1)
    for j in range(10):
        plt.plot(range(iterations), tp_div_support_dict[:,j], label='label {}'.format(j))
    plt.xlabel('Iteration count')
    plt.ylabel('Predicted Accuracy')
    plt.legend()
    plt.savefig('too.png')
    with open("too.png", "rb") as f:
        image_bytes = f.read()
    return base64.b64encode(image_bytes).decode()
   
   